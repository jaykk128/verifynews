---
layout: default
title:  "Verifying News: Blog Post 3"
date:   2017-04-12 19:11:00
categories: main
---
# Verifying News: Blog Post 3

## What's New?
Since our midterm blog post, we've done some exciting new work with our machine learning processes. Previously we had fully implemented SVM machine learning classification to work with on our training set, and this week we have new progress implementing some clustering of news articles. What our goal is here is to be able to identify and group articles with similar MEANING from the features provided in the Webhose data and our own web scraping. Trying to capture actual meaning in computing language is very difficult, and something we will describe below, and finally talk about our thoughts going forwards.

## Clustering
First we researched different papers and read many blogposts regarding clustering articles based on meaning, or containing similar events. One blog post we found that stood out in particular is ![here](blog.chartbeat.com/2015/10/22/identifying-and-clustering-news-events-by-content/). 

The general approach we implemented, with suggestion from the above blog post was to:
1. Separate the article text from each article
2. Use Named Entity Recognition (we used Stanford's NLP NER) to extract significatn entities
3. Vectorize each article's text using the TF-IDF measure
4. Run clustering on these vectors recognizing each article

Here we make the following assumptions:
- We can typically identify a specific event/piece of information by looking at the named/significant entities in a body of an article. In particular we take tagged Location, Person, Organization, Money, Percent, Date, Time from Stanford NLP. 
- TF-IDF vecorization is a good measure of phrase significance in a particular article, that we can then translate into identifying the "meaning" of a particular article.
- More...

## Coding
We implement all this in python, and in particular we used Jupyter Notebook as an interactive and easy way to explore our data.

## Exploring the clustering

 

## Technologies we're using
Technologies we're using: Django webframe, Microsoft SQL Server, Apache Tomcat

## Visualizations
![News Verification Visualization 1]({{ site.url }}/verifynews/assets/59A373AF-0BE9-4549-B272-E93EBC44A506.png)

![News Verification Visualization 2]({{ site.url }}/verifynews/assets/0968D4A8-E3FA-4FCE-A73C-3FB26281F65A.png)

![News Verification Visualization 3]({{ site.url }}/verifynews/assets/489200C6-5AC4-4400-AE45-BEB8696BBB29.png)

![News Verification Visualization 4]({{ site.url }}/verifynews/assets/A2FBEB89-7CF6-4E99-8A93-79F6CA00725C.png)
## Concrete results
We have tens of thousands of news articles, information fully parsed, stored in our cloud-based databases. We've yet to perfect and run our algorithm, but the data is there and ready to be analyzed

## Biggest problems we're facing
Biggest problem? No doubt optimizing our algorithm to handle every imaginable news source and edge case: local news sources, fresh news sources, little-known news sources, sources with poor grammar, sources with unconventional writing styles/diction

## Initial Insights

Our initial insights: fake news verification is an incredibly touchy subject. It is literally impossible to please everyone. We're hoping, however, that we can show our audience that our approach is sufficiently unbiased so as to please liberals and conservatives alike. Further, there's just so much junk data when you're crawling in massive volumes. Most won't notice if a single news article in a hundred turns out completely bogus in our databases, but when you have on the order of tens of thousands, these things really rack up errors. 

## Are we on track?
We are absolutely on track with the project. We're now proceeding on creating the Django webserver.


## Going forward?
Given the initial results, even from our rough-draft algorithm, the results are incredibly promising. We've far exceeded the >70% threshold we were looking for. Things can only improve as we seek more advice on the algorithmic approach from professors and finetune our machine learning model. 
