---
layout: default
title:  "Verifying News: Midterm Blog Post"
date:   2017-03-22 20:04:00
categories: main
---
# Verifying News: Midterm Blog

## Overview

As a part of the broader fake news verification effort, the CrossCheck team is working to crawl massive amounts of news sources online. We have a java apache tomcat client running on Azure actively crawling every five minutes for the most recent news sources on the internet, parsing out relevant information via our endpoint API webhose.io, and storing select, useful information in a cloud-based SQL server. Below is our database schema describing just about every bit of data we're examining. Some questions we're investigating include: spam correlation/source citing correlation with news veracity; News verification as a correlation metric - if other news sources are talking about similar topics, how much more likely is a source to be 'verified'?

## Hardest Parts
Hardest parts of the project include: choosing an appropriate relational dababase schema, parsing the data from websites/news sourcecs, and hosting a server on APACHE Tomcat. 

## Technologies we're using
Technologies we're using: Django webframe, Microsoft SQL Server, Apache Tomcat

## Visualizations

[logo]: https://github.com/verifynews/verifynews/blob/master/_posts/59A373AF-0BE9-4549-B272-E93EBC44A506.png "Logo Title Text 2"
## Concrete results
We have tens of thousands of news articles, information fully parsed, stored in our cloud-based databases. We've yet to perfect and run our algorithm, but the data is there and ready to be analyzed

## Biggest problems we're facing
Biggest problem? No doubt optimizing our algorithm to handle every imaginable news source and edge case: local news sources, fresh news sources, little-known news sources, sources with poor grammar, sources with unconventional writing styles/diction

## Initial Insights

Our initial insights: fake news verification is an incredibly touchy subject. It is literally impossible to please everyone. We're hoping, however, that we can show our audience that our approach is sufficiently unbiased so as to please liberals and conservatives alike. Further, there's just so much junk data when you're crawling in massive volumes. Most won't notice if a single news article in a hundred turns out completely bogus in our databases, but when you have on the order of tens of thousands, these things really rack up errors. 

## Are we on track?
We are absolutely on track with the project. We're now proceeding on creating the Django webserver.


## Going forward?
Given the initial results, even from our rough-draft algorithm, the results are incredibly promising. We've far exceeded the >70% threshold we were looking for. Things can only improve as we seek more advice on the algorithmic approach from professors and finetune our machine learning model. 
